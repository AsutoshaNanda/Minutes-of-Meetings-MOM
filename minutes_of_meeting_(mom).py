# -*- coding: utf-8 -*-
"""Minutes of Meeting (MOM).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vd7mSDFnqV5DhD_3WXbvjBO4rxkHYt6S
"""

!pip install -q --upgrade bitsandbytes accelerate

!pip install -U -q bitsandbytes

import os
import requests
from openai import OpenAI
from google.colab import userdata
from google.colab import drive

from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, BitsAndBytesConfig
import torch
from huggingface_hub import login
from IPython.display import Markdown, display, update_display

LLAMA = "meta-llama/Llama-3.2-3B-Instruct"
AUDIO_MODEL = 'whisper-1'

hf_token = userdata.get('HF_TOKEN_1')
login(hf_token, add_to_git_credential = True)

drive.mount("/content/drive")

audio_file = '/content/drive/MyDrive/Colab Notebooks/HF- Gen_AI/Audio/denver_extract.mp3'

openai_api_key = userdata.get('HF_OPENAI')

openai = OpenAI(api_key = openai_api_key)

# Run it once only.
audio_file = open(audio_file,'rb')

transcription = openai.audio.transcriptions.create(model = "gpt-4o-mini-transcribe",file = audio_file, response_format = 'text')

transcription

display(Markdown(transcription))

transcription_json = openai.audio.transcriptions.create(model = 'gpt-4o-mini-transcribe',file = audio_file, response_format = 'json')

transcription_json

system_message = """
You produce minutes of meetings from transcripts, with summary, key discussion points,
takeaways and action items with owners, in markdown format without code blocks.
"""

user_prompt = f"""
Below is an extract transcript of a Denver council meeting.
Please write minutes in markdown without code blocks, including:
- a summary with attendees, location and date
- discussion points
- takeaways
- action items with owners

Transcription:
{transcription}
"""

messages = [
    {'role':'system','content':system_message},
    {'role':'user','content':user_prompt}
]

from transformers import AutoTokenizer, BitsAndBytesConfig, AutoModelForCausalLM, TextStreamer

quant_config = BitsAndBytesConfig(
    load_in_4bit = True,
    bnb_4bit_use_double_quant = True,
    bnb_4bit_quant_dtype ='nf4',
    bnb_4bit_compute_dtype = torch.bfloat16
)

tokenizer = AutoTokenizer.from_pretrained(LLAMA,trust_remote_code = True)
tokenizer.pod_token = tokenizer.eos_token

inputs = tokenizer.apply_chat_template(messages, return_tensors = 'pt',add_geneartion_prompt = True).to('cuda')

model = AutoModelForCausalLM.from_pretrained(LLAMA, device_map ='auto',quantization_config = quant_config)

streamer = TextStreamer(tokenizer)

outputs = model.generate(inputs, max_new_tokens = 1000,streamer = streamer)

display(Markdown(tokenizer.decode(outputs[0])))

def result():
  return tokenizer.decode(outputs[0])

import gradio as gr

gr.Interface(
    fn = result,
    inputs = None,
    outputs = [gr.Markdown(label = "MOM")],
    allow_flagging = 'never'
).launch()

